---
title: "Predicting Digits from Handwriting"
output: html_notebook
---

Load up the MNIST data from dslabs package as well as the other libraries we will need:

```{r}
library(dslabs) #for the MNIST data

#Helper packages
library(tidyverse) #for tidy data
library(dplyr) #for data wrangling
library(ggplot2) #for visualisations

#Modeling packages

```


Next, read the minst data:

```{r}
mnist <- read_mnist()
```

Let's look at the mnist data:

```{r}
names(mnist)
```

We see there are two components - a training and a test set. Let's look at what they comprise of:


```{r}
names(mnist$train)
names(mnist$test)
```

When we look at these two sets we see a further two components - images and labels. We can check if there are any more subcomponents:

```{r}
names(mnist$train$images)
names(mnist$train$labels)
```

This confirms that there are no further subcomponents. We can next check the class of "images" and "labels" as well:

```{r}
class(mnist$train$images) 
class(mnist$train$labels)
```

This confirms that images component is a matrix/array and labels are classed as integer. Since images component is a matrix, lets check out the dimensions of this matrix:

```{r}
dim(mnist$train$images)
```

Images component is a matrix with n=60,000 and p=784, ie it has 784 features and 60,000 observations.

We can also check the values images component takes:

```{r}
c(min(mnist$train$images), max(mnist$train$images))
```

It looks like 784 features correspond to pixels on a 28x28 frame on which a number is handwritten and the values represent the colour code from 0 to 255. Reading from the online MNIST Documentation (see Readme for the link) we see that 0 pixel value corresponds to background (white), 255 means foreground (black), and that pixels are organised row-wise.

We can also look at the response variable, labels:

```{r}
head(mnist$train$labels)
```

Again, as the online documentation confirms, it is a vector with values from 0 to 9 for assigning the correct label to the handwritten numbers. We can also see how many observations are categorised as each of the integers:

```{r}
table(mnist$train$labels)
```

which looks like a fairly similar distribution.

For computational reasons (we want this example to run on a laptop in a reasonable amount of time - preferrably less than an hour) we will not use the whole 60,000 observations but pick a random sample of 10,000. We will also do the same for the test data.

```{r}
# sample train data
set.seed(123) #for reproduceability
index <- sample(nrow(mnist$train$images), size = 10000) #this will randomly sample 10,000 rows out of the full 60,000
mnist_train_x <- mnist$train$images[index, ] # select those from the images component (explanatory variable or predictor, x) that correspond to the 10,000 selected
mnist_train_y <- mnist$train$labels[index] # select those from labels component (target variable, y) that correspond to the 10,000 selected

# sample test data
index <- sample(nrow(mnist$test$images), size = 10000)
mnist_test_x <- mnist$test$images[index, ]
mnist_test_y <- mnist$test$labels[index]
```


## Preprocessing
In ML, it is often the case that the predictors need to be transformed before running the ML algorithm. Similarly, those predictors that are not useful also need to be removed. All these steps are called pre-processing.

Examples of preprocessing include:

--> standardising the predictors,
--> taking the log transformation of some predictors,
--> removing predictors that are highly correlated with each other,
--> removing predictors with very few non-unique values, or close to zero variation.

Let's look at the last of these as an example. The basic idea of near zero variation is that if little or no variability is present among the predictors then no ink is present in that pixel - ie. no handwriting in that part of the 28x28 box. This is especially true if the corresponding value is 0 since that means it is white - ie no color, just background.

For this we first need to compute the standard deviations of the predictors and plot them. This can be done in two ways:

```{r}
# Option 1 - requires matrixStats package

library(matrixStats)
sds <- colSds(mnist_train_x)
qplot(sds, bins = 256) #alternatively, binwidth = 1

#Option 2  - without matrixStats package

mnist_train_x %>% 
  as.data.frame() %>%
  map_df(sd) %>%
  gather(feature, sd) %>%
  ggplot(aes(sds)) +
    geom_histogram(binwidth = 1) #alternatively, bins = 256
```

From the plot we can see that there are a large number of predictors with zero or near zero variability. This is expected because there are parts of the image that rarely contain writing, very few dark pixels, so there is very little variation, and almost all the values are 0. 

To identify and remove these we need to use the *caret* package.

Caret includes a function that recommends features to be removed due to near zero variance:

```{r}
library(caret)
nzv <- nearZeroVar(mnist_train_x)
```

we can then see the the columns recommended for removal:

```{r}
image(matrix(1:784 %in% nzv, nrow = 28, ncol = 28))
```

To remove the recommended ones and keep the remaining:

```{r}
col_index <- setdiff(1:ncol(mnist_train_x),nzv) #setdiff(x,y) is part of dplyr package. It identifies the differences in x (here x is from 1 to the number of columns in our mnist_train_x which is 784) and y (here y is the net zero values we defined above) by giving output of values that do not overlap between x and y. In other words, the output will be only those numbers that are not common to 1:784 and nzv. 

```

Now we can see how many of the columns, or predictors, we will keep:

```{r}
length(col_index)
```

We see that there are only 249 columns have useful information. We can now remove the other 535 (=784-249) columns with near zero variation. In order to do so, we first need to add column names to the feature matrices - this is a requirement of the *caret* package.

```{r}
colnames_
```

