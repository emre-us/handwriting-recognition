---
title: "Predicting Digits from Handwriting"
output: html_notebook
---

Load up the MNIST data from dslabs package as well as the other libraries we will need:

```{r}
library(dslabs) #for the MNIST data

#Helper packages
library(tidyverse) #for tidy data
library(dplyr) #for data wrangling
library(ggplot2) #for visualisations

#Modeling packages
library(caret) #For machine learning, for resampling and model training, especially kNN
library(randomForest) # for carrying out random forest

```


Next, read the minst data:

```{r}
mnist <- read_mnist()
```

Let's look at the mnist data:

```{r}
names(mnist)
```

We see there are two components - a training and a test set. Let's look at what they comprise of:


```{r}
names(mnist$train)
names(mnist$test)
```

When we look at these two sets we see a further two components - images and labels. We can check if there are any more subcomponents:

```{r}
names(mnist$train$images)
names(mnist$train$labels)
```

This confirms that there are no further subcomponents. We can next check the class of "images" and "labels" as well:

```{r}
class(mnist$train$images) 
class(mnist$train$labels)
```

This confirms that images component is a matrix/array and labels are classed as integer. Since images component is a matrix, lets check out the dimensions of this matrix:

```{r}
dim(mnist$train$images)
```

Images component is a matrix with n=60,000 and p=784, ie it has 784 features and 60,000 observations.

We can also check the values images component takes:

```{r}
c(min(mnist$train$images), max(mnist$train$images))
```

It looks like 784 features correspond to pixels on a 28x28 frame on which a number is handwritten and the values represent the colour code from 0 to 255. Reading from the online MNIST Documentation (see Readme for the link) we see that 0 pixel value corresponds to background (white), 255 means foreground (black), and that pixels are organised row-wise.

We can also look at the response variable, labels:

```{r}
head(mnist$train$labels)
```

Again, as the online documentation confirms, it is a vector with values from 0 to 9 for assigning the correct label to the handwritten numbers. We can also see how many observations are categorised as each of the integers:

```{r}
table(mnist$train$labels)
```

which looks like a fairly similar distribution.

For computational reasons (we want this example to run on a laptop in a reasonable amount of time - preferrably less than an hour) we will not use the whole 60,000 observations but pick a random sample of 10,000. We will also do the same for the test data.

```{r}
# sample train data
set.seed(123) #for reproduceability
index <- sample(nrow(mnist$train$images), size = 10000) #this will randomly sample 10,000 rows out of the full 60,000
x_train <- mnist$train$images[index, ] # select those from the images component (explanatory variable or predictor, x) that correspond to the 10,000 selected
y_train <- factor(mnist$train$labels[index]) # select those from labels component (target variable, y) that correspond to the 10,000 selected. Note that the dependent variables, ie labels, need to be factor for caret. Otherwise when you are running your knn or other algorithm you will get y_train needs to be a factor error.

# sample test data
index <- sample(nrow(mnist$test$images), size = 10000)
x_test <- mnist$test$images[index, ]
y_test <- factor(mnist$test$labels[index])
```


## Preprocessing
In ML, it is often the case that the predictors need to be transformed before running the ML algorithm. Similarly, those predictors that are not useful also need to be removed. All these steps are called pre-processing.

Examples of preprocessing include:

--> standardising the predictors,
--> taking the log transformation of some predictors,
--> removing predictors that are highly correlated with each other,
--> removing predictors with very few non-unique values, or close to zero variation.

Let's look at the last of these as an example. The basic idea of near zero variation is that if little or no variability is present among the predictors then no ink is present in that pixel - ie. no handwriting in that part of the 28x28 box. This is especially true if the corresponding value is 0 since that means it is white - ie no color, just background.

For this we first need to compute the standard deviations of the predictors and plot them. This can be done in two ways:

```{r}
# Option 1 - requires matrixStats package

library(matrixStats)
sds <- colSds(x_train)
qplot(sds, bins = 256) #alternatively, binwidth = 1

#Option 2  - without matrixStats package

x_train %>% 
  as.data.frame() %>%
  map_df(sd) %>%
  gather(feature, sd) %>%
  ggplot(aes(sds)) +
    geom_histogram(binwidth = 1) #alternatively, bins = 256
```

From the plot we can see that there are a large number of predictors with zero or near zero variability. This is expected because there are parts of the image that rarely contain writing, very few dark pixels, so there is very little variation, and almost all the values are 0. 

To identify and remove these we need to use the *caret* package.

Caret includes a function that recommends features to be removed due to near zero variance:

```{r}
nzv <- nearZeroVar(x_train)
```

we can then see the the columns recommended for removal:

```{r}
image(matrix(1:784 %in% nzv, nrow = 28, ncol = 28))
```

To remove the recommended ones and keep the remaining:

```{r}
col_index <- setdiff(1:ncol(x_train),nzv) #setdiff(x,y) is part of dplyr package. It identifies the differences in x (here x is from 1 to the number of columns in our mnist_train_x which is 784) and y (here y is the net zero values we defined above) by giving output of values that do not overlap between x and y. In other words, the output will be only those numbers that are not common to 1:784 and nzv. 

```

Now we can see how many of the columns, or predictors, we will keep:

```{r}
length(col_index)
```

We see that there are only 249 columns have useful information. We can now remove the other 535 (=784-249) columns with near zero variation. Before doing so, make sure to add column names to the feature matrices - this is a requirement of the *caret* package for fitting models. If we name the columns after we remove the nzv ones, then when we apply the same names to test predictors, there will be a dimensional mismatch since only 249 will have names in the former, and 784 in the latter.

We will add the column numbers from images component as the names for column names. To add names to columns of features:

```{r}
#Rename features:
colnames(x_train) <- 1:ncol(mnist$train$images)

#Apply the column names to the predictors in the test set as well:
colnames(x_test) <- colnames(x_test)
```



## Modelling

Now we are ready for modelling. We will start with a very simple but widely used algorithm - kNN. 


### kNN

This is an algo in which each observation is predicted based on its 'similarity' to other observations. It is considered a lazy learner because it is not really 'learning' anything. It is a memory-based algo and relies on training samples and not on abstracted model. One of the downsides of this is that the process of making predictions tend to be relatively slow and computationally inefficient. Nevertheless, this is a powerful algo that can be very useful.

The kNN algo identifies k observations that are nearest to the new record being predicted (ie unlabeled) and then uses the average response value (regression) or the most common class (classification) of those k observations as the predicted output.

The first step is to choose k. After choosing k, the algo requires a training data set classified into several categories as labeled by a nominal variable (in our case from 0 to 9 to identify the handwritten numbers).

The distance for determining the 'nearest' or 'similarity' can be determined in a number of ways. The two most common are Euclidean and Manhattan (others include Minkowski and Mahalanobis distances). Former measures the straight line distance (as the crow flies), while the latter is based on the paths a pedestrian would take around city blocks. It effectively measures the point-to-point travel time (ie city block) and is commonly used for binary predictors. 

The distance between $x_a$ and $x_b$ for all j features can be calculated as:

$$
Euclidean = \sqrt{\sum_{j=1}^{P}{(x_{aj} - x_{bj})^2}}
$$

$$
Manhattan = \sum_{j=1}^{P}{|x_{aj} - x_{bj}|}
$$


We will use Euclidean here since we don't have binary encoding. You can get more info on distance via the documentation of R's distance function using the ?dist command.

The performance of kNN is very sensitive to the choice of k. There is no general rule about the best k as it depends on the nature of the data. For data with lots of noisy (irrelevant) features, larger k tends to be required to smooth out the noise.

Given that we have to compute distance between each obesrvation in the training and test sets, this will have a large computational demand. We will use k-fold cross validation to optimise k and improve speed via caret package. This will find the model that will maximise the accuracy. 

As a way of illustration, if you run the following code that attempts to see the effect of k= 3, 5, or 7, it would take several minutes.

```{r}
# train_knn <- train(x_train, 
#                    y_train,
#                    method = "knn",
#                    tuneGrid = data.frame(k=c(3,5,7)),   # create a hyperparameter grid search
#                    trControl = trainControl(method = "cv",   # resampling
#                                             number = 10,
#                                             p = 0.9))
```

Therefore, it is a good practice to try and test out a piece of code on a subset of the data first in order to get an idea of the computational time. 

To do this, we first define n as the number of rows we will use in this subset of the data and b as the number of cross-validation folds. We can then increase the number slowly to get an idea how long the final code will take.

```{r}
n <- 1000
b <- 2
set.seed(123) #for reproduceability
index <- sample(nrow(x_train), n)

#depending on the order in which libraries have been loaded dply may mask caret's train function. to avoid getting an error, it may be prudent to specify the package from which you are calling the train() function

train_knn <- caret::train(x_train[index,],   
                   y_train[index],
                   method = "knn",
                   tuneGrid = data.frame(k = c(3,5,7)),
                   trControl = trainControl(method = "cv", 
                                            number = b,
                                            p = 0.9))
train_knn
train_knn$bestTune
plot(train_knn)
```

So we can see that when k = 3 we have the lowest level of RMSE. We can then use that to fit it to the entire data set, and then check its prediction accuracy:

```{r}
# fit the knn model using caret's knn3 (alternatively you can use class package's knn() function)
fit_knn <- knn3(x_train, y_train, k=3) #if you don't do as.factor(train_y) you will get y needs to be a factor error. Alternatively, you can set train_y as factor above at the very beginning
```

we can then check the accuracy
```{r}
# assign the prediction model to y_hat. 
y_hat_knn <- predict(fit_knn,
                     x_test,
                     type="class")

#To check accuracy via Confusion Matrix
confusionMatrix(y_hat_knn, y_test)  #if you want to check just the accuracy then add $overall["Accuracy"] to the end of this code

```

Which gives a pretty good accuracy of almost 95%.

From sensitivity and specificity we can see which digits are the hardest to detect (sensitivity) and which ones are the most commonly incorrectly predicted digit (specificity). From the table it seems like 8s are the hardest to detect since they have the lowest sensitivity. 9s are the most commonly incorrectly predicted digit since they have the lowest specificity.

Can we do better? Let's try random forest algorithm.



### Random Forest

Random forests create a challenge for us in terms of computing time. This is because for each forest we need to build hundreds of trees. We also have several parameters we can tune. In kNN it is the predicting that can be time-intensive, whereas with random forests it is the fitting that is the slowest part of the procedure. Due to this slowness, we will use only 5-fold cross validation and we will reduce the number of trees that are fit since we are not yet building our final model. We will also use a smaller data set to compute by taking a random sample of the observations when constructing the tree.

We can change this number with the nSamp argument in caret's train function. The code is similar to kNN but notice that the method is "rf" now. The code below will take several minutes to run:

```{r}
#as was the case with kNN we will use the train() function
train_rf <- caret::train(x_train, 
                         y_train,
                         method = "rf",
                         ntree = 150,
                         trControl = trainControl(method = "cv",
                                                  number = 5),
                         tuneGrid = data.frame(mtry = c(1,5,10,25,50,100)),
                         nSamp = 100)

# plot the final result and find the best tuning point that minimises RMSE
train_rf
ggplot(train_rf)
train_rf$bestTune
```

Now we have optimised our algo, we are ready to fit our final model (also will take several minutes to run):

```{r}
#fit the model using randomForest() function

fit_rf <- randomForest(x_train, 
                       y_train,
                       minNode = train_rf$bestTune$mtry)

# plot the model to check we ran enough trees

plot(fit_rf)

```

As before, we can check the accuracy of our random forest algo:

```{r}
y_hat_rf <- predict(fit_rf,
                    x_test)
confusionMatrix(y_hat_rf, y_test)
```

